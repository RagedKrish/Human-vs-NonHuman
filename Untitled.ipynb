{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c5d3c3-b9d0-4eea-afb0-4f89f5d2efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  # For image processing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d47770-86fe-4d2b-a33b-877d1ca7210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_folder = '1/'\n",
    "nohuman_folder = '0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a46850-6e73-4cdd-9076-507bda9dc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human images: 100%|██████████| 1061/1061 [00:00<00:00, 353832.91it/s]\n",
      "Loading no human images: 100%|██████████| 2306/2306 [00:00<00:00, 266118.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load human images and assign label 1\n",
    "for img_name in tqdm(os.listdir(human_folder), desc=\"Loading human images\"):\n",
    "    img_path = os.path.join(human_folder, img_name)\n",
    "    if img_name is not None:\n",
    "        images.append(img_path)\n",
    "        labels.append(1)  # Label 1 for human\n",
    "\n",
    "# Load no human images and assign label 0\n",
    "for img_name in tqdm(os.listdir(nohuman_folder), desc=\"Loading no human images\"):\n",
    "    img_path = os.path.join(nohuman_folder, img_name)\n",
    "    if img_name is not None:\n",
    "        images.append(img_path)\n",
    "        labels.append(0)  # Label 0 for no human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00b51a7-26d4-4b04-b4b6-131324a9a04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/0.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1 (680).jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1 (6800).jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1 (6801).jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1 (6802).jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>0/Zebra_9.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>0/Zebra_9_1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>0/Zebra_9_2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>0/Zebra_9_3.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>0/Zebra_9_4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3367 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filepath  label\n",
       "0             1/0.png      1\n",
       "1       1/1 (680).jpg      1\n",
       "2      1/1 (6800).jpg      1\n",
       "3      1/1 (6801).jpg      1\n",
       "4      1/1 (6802).jpg      1\n",
       "...               ...    ...\n",
       "3362   0/Zebra_9.jpeg      0\n",
       "3363  0/Zebra_9_1.jpg      0\n",
       "3364  0/Zebra_9_2.jpg      0\n",
       "3365  0/Zebra_9_3.jpg      0\n",
       "3366  0/Zebra_9_4.jpg      0\n",
       "\n",
       "[3367 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df = pd.DataFrame({'filepath': images, 'label': labels})\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1778ee-fc4b-414a-9369-7c2fcd01cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53019e2-0c0c-4dac-898d-b862df38e770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/0.png</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1 (680).jpg</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1 (6800).jpg</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1 (6801).jpg</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1 (6802).jpg</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>0/Zebra_9.jpeg</td>\n",
       "      <td>no_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>0/Zebra_9_1.jpg</td>\n",
       "      <td>no_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>0/Zebra_9_2.jpg</td>\n",
       "      <td>no_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>0/Zebra_9_3.jpg</td>\n",
       "      <td>no_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>0/Zebra_9_4.jpg</td>\n",
       "      <td>no_human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3367 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filepath     label\n",
       "0             1/0.png     human\n",
       "1       1/1 (680).jpg     human\n",
       "2      1/1 (6800).jpg     human\n",
       "3      1/1 (6801).jpg     human\n",
       "4      1/1 (6802).jpg     human\n",
       "...               ...       ...\n",
       "3362   0/Zebra_9.jpeg  no_human\n",
       "3363  0/Zebra_9_1.jpg  no_human\n",
       "3364  0/Zebra_9_2.jpg  no_human\n",
       "3365  0/Zebra_9_3.jpg  no_human\n",
       "3366  0/Zebra_9_4.jpg  no_human\n",
       "\n",
       "[3367 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df['label'] = image_df['label'].map({1: 'human', 0: 'no_human'})\n",
    "image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fb22e8-07e4-4ff8-a1d2-1aa377c237c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2694 validated image filenames belonging to 2 classes.\n",
      "Found 673 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set image dimensions and batch size\n",
    "img_height, img_width = 299, 299\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=image_df,\n",
    "    x_col='filepath',            # Column with image paths\n",
    "    y_col='label',               # Column with labels\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',         # Binary labels\n",
    "    subset='training',           # Use as training data\n",
    "    shuffle=True                 # Shuffle data for training\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=image_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation',         # Use as validation data\n",
    "    shuffle=False                # No shuffle for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd16732-957d-4cee-9d6c-5ecc4242fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications import Xception\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c519fb0-7631-4efa-9c9b-ee6015cb03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base Xception model, excluding the top layers\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the base model's layers to retain pretrained features\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  # Fully connected layer\n",
    "output = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06eef4a-e0d3-44cb-8ffa-a94c3efbe835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b665841b-3b8b-47ff-bd64-cdd1372d51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deepLearning\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8379 - loss: 0.3539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deepLearning\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "C:\\ProgramData\\anaconda3\\envs\\deepLearning\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 3s/step - accuracy: 0.8385 - loss: 0.3527 - val_accuracy: 0.1176 - val_loss: 2.1285\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/84\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 2s/step - accuracy: 0.9688 - loss: 0.1713"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deepLearning\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1713 - val_accuracy: 0.0000e+00 - val_loss: 0.7647\n",
      "Epoch 3/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 3s/step - accuracy: 0.9422 - loss: 0.1364 - val_accuracy: 0.1845 - val_loss: 1.7441\n",
      "Epoch 4/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1087 - val_accuracy: 1.0000 - val_loss: 0.2980\n",
      "Epoch 5/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 3s/step - accuracy: 0.9459 - loss: 0.1223 - val_accuracy: 0.2307 - val_loss: 1.5403\n",
      "Epoch 6/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1124 - val_accuracy: 1.0000 - val_loss: 0.1442\n",
      "Epoch 7/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 3s/step - accuracy: 0.9635 - loss: 0.0935 - val_accuracy: 0.2202 - val_loss: 1.7346\n",
      "Epoch 8/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0304 - val_accuracy: 1.0000 - val_loss: 0.2666\n",
      "Epoch 9/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 3s/step - accuracy: 0.9614 - loss: 0.0856 - val_accuracy: 0.2634 - val_loss: 1.7191\n",
      "Epoch 10/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0266 - val_accuracy: 1.0000 - val_loss: 0.1520\n",
      "Epoch 11/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 3s/step - accuracy: 0.9677 - loss: 0.0804 - val_accuracy: 0.2470 - val_loss: 1.8759\n",
      "Epoch 12/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9688 - loss: 0.0623 - val_accuracy: 1.0000 - val_loss: 0.1208\n",
      "Epoch 13/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 3s/step - accuracy: 0.9649 - loss: 0.0860 - val_accuracy: 0.3289 - val_loss: 1.6051\n",
      "Epoch 14/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9062 - loss: 0.1331 - val_accuracy: 1.0000 - val_loss: 0.0704\n",
      "Epoch 15/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 3s/step - accuracy: 0.9754 - loss: 0.0603 - val_accuracy: 0.4821 - val_loss: 1.1596\n",
      "Epoch 16/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.0908 - val_accuracy: 1.0000 - val_loss: 0.0147\n",
      "Epoch 17/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 3s/step - accuracy: 0.9723 - loss: 0.0730 - val_accuracy: 0.2753 - val_loss: 1.8923\n",
      "Epoch 18/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 0.0976\n",
      "Epoch 19/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 3s/step - accuracy: 0.9783 - loss: 0.0642 - val_accuracy: 0.2426 - val_loss: 2.1502\n",
      "Epoch 20/20\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9688 - loss: 0.0524 - val_accuracy: 1.0000 - val_loss: 0.0474\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,  # Adjust based on performance and resources\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=val_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447b63ee-002e-449a-bae7-f083fb2d5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m850s\u001b[0m 9s/step - accuracy: 0.9552 - loss: 0.1117 - val_accuracy: 0.3795 - val_loss: 1.6510\n",
      "Epoch 2/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 1.0000 - val_loss: 0.0201\n",
      "Epoch 3/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 9s/step - accuracy: 0.9685 - loss: 0.0760 - val_accuracy: 0.3839 - val_loss: 1.7696\n",
      "Epoch 4/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0710 - val_accuracy: 1.0000 - val_loss: 0.0200\n",
      "Epoch 5/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m784s\u001b[0m 9s/step - accuracy: 0.9802 - loss: 0.0518 - val_accuracy: 0.3348 - val_loss: 1.9599\n",
      "Epoch 6/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0213\n",
      "Epoch 7/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 9s/step - accuracy: 0.9863 - loss: 0.0470 - val_accuracy: 0.3199 - val_loss: 2.0636\n",
      "Epoch 8/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0093\n",
      "Epoch 9/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 10s/step - accuracy: 0.9885 - loss: 0.0324 - val_accuracy: 0.3497 - val_loss: 1.9550\n",
      "Epoch 10/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 0.0076\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,  # Additional epochs for fine-tuning\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=val_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9623c6-2285-45e4-85d7-cd93d69c9c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 4/21\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 2s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deepLearning\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step\n",
      "Mismatch in the number of predictions and true labels!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = model.predict(val_generator, steps=val_generator.samples // val_generator.batch_size)\n",
    "\n",
    "# Convert probabilities to binary labels (threshold at 0.5)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Get the true labels from the generator (this should match the number of predictions)\n",
    "true_labels = val_generator.classes\n",
    "\n",
    "# Ensure that both arrays are of the same length\n",
    "if len(true_labels) == len(predicted_labels):\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    print(f\"Manual Accuracy: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Mismatch in the number of predictions and true labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ba7974-430f-4db9-a9c6-323693414d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.1842 - loss: 2.4681\n",
      "Validation Accuracy: 0.3507\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2f0ea7-2d30-4930-8451-8a399c928288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('human_identifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cce4cc93-fbdf-456b-b889-7241f05559f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('human_identifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7a2490-0bac-4630-901e-7faa0691907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d896d3f6-efc5-4eff-8c89-8b38632c066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19c6501-9a5b-4200-a00b-bb34bd128396",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('1/406.png')\n",
    "imgsize=[299,299]\n",
    "img=cv2.resize(img,imgsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2d0b0e2-a2c8-4ba3-b26c-f9c2d2470382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a0dbd69-7fa2-4acb-836b-5c857ab2d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=img_to_array(img)\n",
    "i=i.reshape(-1,299,299,3)\n",
    "i=i/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63389191-47f0-40ff-946e-2bb316ba629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00128096]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808b5e2-bf06-4783-9e49-4568e91fad8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
